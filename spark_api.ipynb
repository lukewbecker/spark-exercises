{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Dataframes\n",
    "\n",
    "- look like pandas dataframes\n",
    "- share some of the same methods and syntax\n",
    "- but they are 2 seperate types of objects\n",
    "\n",
    "**Create Spark Session**\n",
    "\n",
    "`pyspark.sql.SparkSession.builder.getOrCreate()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "\n",
    "#method to create spark session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataframes\n",
    "\n",
    "### From a Pandas Dataframe\n",
    "\n",
    "First, we create the pandas dataframe. As a reminder, there are multiple ways to create a pandas dataframe. \n",
    "\n",
    "1. From a **dictionary-like object**, where we provide the values by columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r1c1</td>\n",
       "      <td>r1c2</td>\n",
       "      <td>r1c3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r2c1</td>\n",
       "      <td>r2c2</td>\n",
       "      <td>r3c3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r3c1</td>\n",
       "      <td>r3c2</td>\n",
       "      <td>r3c3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1  col2  col3\n",
       "1  r1c1  r1c2  r1c3\n",
       "2  r2c1  r2c2  r3c3\n",
       "3  r3c1  r3c2  r3c3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create pandas dataframe by columns using dictionary-like object\n",
    "\n",
    "pd_df = pd.DataFrame({'col1': ['r1c1', 'r2c1', 'r3c1'],\n",
    "                      'col2': ['r1c2', 'r2c2', 'r3c2'],\n",
    "                      'col3': ['r1c3', 'r3c3', 'r3c3']\n",
    "                        }, \n",
    "                     index = [1, 2, 3])\n",
    "pd_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. From an **array-like object**, where we provide values by rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r1c1</td>\n",
       "      <td>r1c2</td>\n",
       "      <td>r1c3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r2c1</td>\n",
       "      <td>r2c2</td>\n",
       "      <td>r2c3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r3c1</td>\n",
       "      <td>r3c2</td>\n",
       "      <td>r3c3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1  col2  col3\n",
       "1  r1c1  r1c2  r1c3\n",
       "2  r2c1  r2c2  r2c3\n",
       "3  r3c1  r3c2  r3c3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create pandas dataframe by rows\n",
    "pd_df = pd.DataFrame([['r1c1', 'r1c2', 'r1c3'], \n",
    "                      ['r2c1', 'r2c2', 'r2c3'], \n",
    "                      ['r3c1', 'r3c2', 'r3c3']\n",
    "                      ], \n",
    "                     index = [1, 2, 3], \n",
    "                     columns = ['col1', 'col2', 'col3'])\n",
    "\n",
    "pd_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, **create the Spark dataframe** from Pandas dataframe using `spark.createDataFrame()`. `spark` is referring to the session we created in the beginning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[col1: string, col2: string, col3: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_df = spark.createDataFrame(pd_df)\n",
    "sp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**View the spark dataframe**\n",
    "\n",
    "- spark is lazy\n",
    "- .show (defaults to 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col1|col2|col3|\n",
      "+----+----+----+\n",
      "|r1c1|r1c2|r1c3|\n",
      "|r2c1|r2c2|r2c3|\n",
      "|r3c1|r3c2|r3c3|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from Files\n",
    "\n",
    "Comparing pandas and spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pandas</th>\n",
       "      <th>spark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>csv</th>\n",
       "      <td>pd.read_csv(\"myfile.csv\")</td>\n",
       "      <td>spark.read.load(\"myfile.csv\", format = \"csv\", sep = \",\")</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>json</th>\n",
       "      <td>pd.read_json(\"myfile.json\")</td>\n",
       "      <td>spark.read.load(\"myfile.json\", format = \"json\") OR spark.read.json(\"myfile.json\")</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           pandas  \\\n",
       "csv     pd.read_csv(\"myfile.csv\")   \n",
       "json  pd.read_json(\"myfile.json\")   \n",
       "\n",
       "                                                                                  spark  \n",
       "csv                            spark.read.load(\"myfile.csv\", format = \"csv\", sep = \",\")  \n",
       "json  spark.read.load(\"myfile.json\", format = \"json\") OR spark.read.json(\"myfile.json\")  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_v_spark = pd.DataFrame([['pd.read_csv(\"myfile.csv\")', \n",
    "                            'spark.read.load(\"myfile.csv\", format = \"csv\", sep = \",\")'], \n",
    "                           ['pd.read_json(\"myfile.json\")', \n",
    "                            'spark.read.load(\"myfile.json\", format = \"json\") OR spark.read.json(\"myfile.json\")']], \n",
    "                          index = ['csv', 'json'], \n",
    "                          columns = ['pandas', 'spark'])\n",
    "\n",
    "# to display and see all text in dataframe\n",
    "pd.set_option('display.max_colwidth', 10000)\n",
    "\n",
    "\n",
    "pd_v_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize dataframes\n",
    "\n",
    "Comparing pandas and spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_v_spark = pd_v_spark.append(pd.DataFrame([['pd_df.head()', 'sp_df.show(), .head(), .take()'],\n",
    "                                             ['pd_df.head(1)', 'sp_df.first()'],\n",
    "                                             ['pd_df.describe()', 'sp_df.describe()'],\n",
    "                                             ['pd_df.columns', 'sp_df.columns'],\n",
    "                                             ['len(pd_df)', 'sp_df.count()'],\n",
    "                                             ['len(pd_df.drop_duplicates())', 'sp_df.distinct().count()'],\n",
    "                                             ['pd_df.info()', 'sp_df.printSchema()']\n",
    "                                            ],\n",
    "                                            index = ['1st n rows', '1st row','summary statistics', \n",
    "                                                     'column names', '# rows', '# distinct rows', \n",
    "                                                     'df schema info'], \n",
    "                                            columns = ['pandas', 'spark']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pandas</th>\n",
       "      <th>spark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>csv</th>\n",
       "      <td>pd.read_csv(\"myfile.csv\")</td>\n",
       "      <td>spark.read.load(\"myfile.csv\", format = \"csv\", sep = \",\")</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>json</th>\n",
       "      <td>pd.read_json(\"myfile.json\")</td>\n",
       "      <td>spark.read.load(\"myfile.json\", format = \"json\") OR spark.read.json(\"myfile.json\")</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st n rows</th>\n",
       "      <td>pd_df.head()</td>\n",
       "      <td>sp_df.show(), .head(), .take()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st row</th>\n",
       "      <td>pd_df.head(1)</td>\n",
       "      <td>sp_df.first()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summary statistics</th>\n",
       "      <td>pd_df.describe()</td>\n",
       "      <td>sp_df.describe()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column names</th>\n",
       "      <td>pd_df.columns</td>\n",
       "      <td>sp_df.columns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># rows</th>\n",
       "      <td>len(pd_df)</td>\n",
       "      <td>sp_df.count()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># distinct rows</th>\n",
       "      <td>len(pd_df.drop_duplicates())</td>\n",
       "      <td>sp_df.distinct().count()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>df schema info</th>\n",
       "      <td>pd_df.info()</td>\n",
       "      <td>sp_df.printSchema()</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          pandas  \\\n",
       "csv                    pd.read_csv(\"myfile.csv\")   \n",
       "json                 pd.read_json(\"myfile.json\")   \n",
       "1st n rows                          pd_df.head()   \n",
       "1st row                            pd_df.head(1)   \n",
       "summary statistics              pd_df.describe()   \n",
       "column names                       pd_df.columns   \n",
       "# rows                                len(pd_df)   \n",
       "# distinct rows     len(pd_df.drop_duplicates())   \n",
       "df schema info                      pd_df.info()   \n",
       "\n",
       "                                                                                                spark  \n",
       "csv                                          spark.read.load(\"myfile.csv\", format = \"csv\", sep = \",\")  \n",
       "json                spark.read.load(\"myfile.json\", format = \"json\") OR spark.read.json(\"myfile.json\")  \n",
       "1st n rows                                                             sp_df.show(), .head(), .take()  \n",
       "1st row                                                                                 sp_df.first()  \n",
       "summary statistics                                                                   sp_df.describe()  \n",
       "column names                                                                            sp_df.columns  \n",
       "# rows                                                                                  sp_df.count()  \n",
       "# distinct rows                                                              sp_df.distinct().count()  \n",
       "df schema info                                                                    sp_df.printSchema()  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_v_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a dataset with more realistic looking data to explore..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>model</th>\n",
       "      <th>displ</th>\n",
       "      <th>year</th>\n",
       "      <th>cyl</th>\n",
       "      <th>trans</th>\n",
       "      <th>drv</th>\n",
       "      <th>cty</th>\n",
       "      <th>hwy</th>\n",
       "      <th>fl</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>audi</td>\n",
       "      <td>a4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1999</td>\n",
       "      <td>4</td>\n",
       "      <td>auto(l5)</td>\n",
       "      <td>f</td>\n",
       "      <td>18</td>\n",
       "      <td>29</td>\n",
       "      <td>p</td>\n",
       "      <td>compact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>audi</td>\n",
       "      <td>a4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1999</td>\n",
       "      <td>4</td>\n",
       "      <td>manual(m5)</td>\n",
       "      <td>f</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "      <td>p</td>\n",
       "      <td>compact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>audi</td>\n",
       "      <td>a4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>manual(m6)</td>\n",
       "      <td>f</td>\n",
       "      <td>20</td>\n",
       "      <td>31</td>\n",
       "      <td>p</td>\n",
       "      <td>compact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>audi</td>\n",
       "      <td>a4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>auto(av)</td>\n",
       "      <td>f</td>\n",
       "      <td>21</td>\n",
       "      <td>30</td>\n",
       "      <td>p</td>\n",
       "      <td>compact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>audi</td>\n",
       "      <td>a4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1999</td>\n",
       "      <td>6</td>\n",
       "      <td>auto(l5)</td>\n",
       "      <td>f</td>\n",
       "      <td>16</td>\n",
       "      <td>26</td>\n",
       "      <td>p</td>\n",
       "      <td>compact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  manufacturer model  displ  year  cyl       trans drv  cty  hwy fl    class\n",
       "1         audi    a4    1.8  1999    4    auto(l5)   f   18   29  p  compact\n",
       "2         audi    a4    1.8  1999    4  manual(m5)   f   21   29  p  compact\n",
       "3         audi    a4    2.0  2008    4  manual(m6)   f   20   31  p  compact\n",
       "4         audi    a4    2.0  2008    4    auto(av)   f   21   30  p  compact\n",
       "5         audi    a4    2.8  1999    6    auto(l5)   f   16   26  p  compact"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydataset import data\n",
    "\n",
    "mpg_pd = data(\"mpg\")\n",
    "mpg_pd.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas df to spark df..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|manufacturer|model|displ|year|cyl|     trans|drv|cty|hwy| fl|  class|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|        audi|   a4|  1.8|1999|  4|  auto(l5)|  f| 18| 29|  p|compact|\n",
      "|        audi|   a4|  1.8|1999|  4|manual(m5)|  f| 21| 29|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|manual(m6)|  f| 20| 31|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|  auto(av)|  f| 21| 30|  p|compact|\n",
      "|        audi|   a4|  2.8|1999|  6|  auto(l5)|  f| 16| 26|  p|compact|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg = spark.createDataFrame(data(\"mpg\"))\n",
    "mpg.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns\n",
    "\n",
    "Pandas series vs. Spark column objects\n",
    "\n",
    "A column object represents a vertical slice of a dataframe, but does not contain the data itself. \n",
    "\n",
    "You will use it to perform functions on and reference that column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1          a4\n",
       "2          a4\n",
       "3          a4\n",
       "4          a4\n",
       "5          a4\n",
       "        ...  \n",
       "230    passat\n",
       "231    passat\n",
       "232    passat\n",
       "233    passat\n",
       "234    passat\n",
       "Name: model, Length: 234, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandas series\n",
    "mpg_pd.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'model'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark column object\n",
    "mpg.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select columns**\n",
    "\n",
    "Comparing pandas and spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pandas</th>\n",
       "      <th>spark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>csv</th>\n",
       "      <td>pd.read_csv(\"myfile.csv\")</td>\n",
       "      <td>spark.read.load(\"myfile.csv\", format = \"csv\", sep = \",\")</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>json</th>\n",
       "      <td>pd.read_json(\"myfile.json\")</td>\n",
       "      <td>spark.read.load(\"myfile.json\", format = \"json\") OR spark.read.json(\"myfile.json\")</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st n rows</th>\n",
       "      <td>pd_df.head()</td>\n",
       "      <td>sp_df.show(), .head(), .take()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st row</th>\n",
       "      <td>pd_df.head(1)</td>\n",
       "      <td>sp_df.first()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summary statistics</th>\n",
       "      <td>pd_df.describe()</td>\n",
       "      <td>sp_df.describe()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column names</th>\n",
       "      <td>pd_df.columns</td>\n",
       "      <td>sp_df.columns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># rows</th>\n",
       "      <td>len(pd_df)</td>\n",
       "      <td>sp_df.count()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># distinct rows</th>\n",
       "      <td>len(pd_df.drop_duplicates())</td>\n",
       "      <td>sp_df.distinct().count()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>df schema info</th>\n",
       "      <td>pd_df.info()</td>\n",
       "      <td>sp_df.printSchema()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>select columns</th>\n",
       "      <td>pd_df[[\"col1\", \"col2\"]]</td>\n",
       "      <td>sp_df.select(sp_df.col1, sp_df.col2)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          pandas  \\\n",
       "csv                    pd.read_csv(\"myfile.csv\")   \n",
       "json                 pd.read_json(\"myfile.json\")   \n",
       "1st n rows                          pd_df.head()   \n",
       "1st row                            pd_df.head(1)   \n",
       "summary statistics              pd_df.describe()   \n",
       "column names                       pd_df.columns   \n",
       "# rows                                len(pd_df)   \n",
       "# distinct rows     len(pd_df.drop_duplicates())   \n",
       "df schema info                      pd_df.info()   \n",
       "select columns           pd_df[[\"col1\", \"col2\"]]   \n",
       "\n",
       "                                                                                                spark  \n",
       "csv                                          spark.read.load(\"myfile.csv\", format = \"csv\", sep = \",\")  \n",
       "json                spark.read.load(\"myfile.json\", format = \"json\") OR spark.read.json(\"myfile.json\")  \n",
       "1st n rows                                                             sp_df.show(), .head(), .take()  \n",
       "1st row                                                                                 sp_df.first()  \n",
       "summary statistics                                                                   sp_df.describe()  \n",
       "column names                                                                            sp_df.columns  \n",
       "# rows                                                                                  sp_df.count()  \n",
       "# distinct rows                                                              sp_df.distinct().count()  \n",
       "df schema info                                                                    sp_df.printSchema()  \n",
       "select columns                                                   sp_df.select(sp_df.col1, sp_df.col2)  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_v_spark = pd_v_spark.append(pd.DataFrame([['pd_df[[\"col1\", \"col2\"]]', \n",
    "                                              'sp_df.select(sp_df.col1, sp_df.col2)']\n",
    "                                            ],\n",
    "                                            index = ['select columns'], \n",
    "                                            columns = ['pandas', 'spark']))\n",
    "pd_v_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select columns hwy, cty, and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hwy: bigint, cty: bigint, model: string]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg.select(mpg.hwy, mpg.cty, mpg.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can I show the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+\n",
      "|hwy|cty|model|\n",
      "+---+---+-----+\n",
      "| 29| 18|   a4|\n",
      "| 29| 21|   a4|\n",
      "| 31| 20|   a4|\n",
      "| 30| 21|   a4|\n",
      "| 26| 16|   a4|\n",
      "+---+---+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(mpg.hwy, mpg.cty, mpg.model).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column objects support operations such as arithmetic operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'(hwy + 1)'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg.hwy + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "|hwy|(hwy + 1)|\n",
      "+---+---------+\n",
      "| 29|       30|\n",
      "| 29|       30|\n",
      "| 31|       32|\n",
      "+---+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(mpg.hwy, mpg.hwy+1).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a column object, we can use the .alias method to rename it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------------+\n",
      "|highway_mileage|highway_mileage_plus1|\n",
      "+---------------+---------------------+\n",
      "|             29|                   30|\n",
      "|             29|                   30|\n",
      "|             31|                   32|\n",
      "+---------------+---------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(mpg.hwy.alias('highway_mileage'), \n",
    "           (mpg.hwy+1).alias('highway_mileage_plus1')).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also store column objects in variables and reference them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------------+\n",
      "|highway_mileage|highway_mileage_halved|\n",
      "+---------------+----------------------+\n",
      "|             29|                  14.5|\n",
      "|             29|                  14.5|\n",
      "|             31|                  15.5|\n",
      "+---------------+----------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "col1 = mpg.hwy.alias('highway_mileage')\n",
    "col2 = (mpg.hwy/2).alias('highway_mileage_halved')\n",
    "\n",
    "mpg.select(col1, col2).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the syntax we've seen above, we can create columns with the `col` and `expr` functions from `pyspark.sql.functions` module.\n",
    "\n",
    "**col**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'hwy'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "col(\"hwy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'hwy'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg.hwy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column object produced by the col function is the same as the the previous column object we saw.\n",
    "\n",
    "We can create `avg_mileage` using the col function to produce pyspark Column objects and using the arithmetic operators to combine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'((hwy + cty) / 2)'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_col = (col(\"hwy\") + col(\"cty\")) / 2\n",
    "avg_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----------+\n",
      "|hwy_mileage|cty_mileage|avg_mileage|\n",
      "+-----------+-----------+-----------+\n",
      "|         29|         18|       23.5|\n",
      "|         29|         21|       25.0|\n",
      "|         31|         20|       25.5|\n",
      "+-----------+-----------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(\n",
    "    col(\"hwy\").alias(\"hwy_mileage\"), \n",
    "    mpg.cty.alias(\"cty_mileage\"),\n",
    "    avg_col.alias(\"avg_mileage\")\n",
    ").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**expr**\n",
    "\n",
    "- Does everything col does and more\n",
    "- Returns the same type of column object\n",
    "- But also allows us to express manipulations to the column within the string that defines the column.\n",
    "- Which syntax to use is merely a style choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----------+---------------+\n",
      "|hwy|(hwy + 1)|hwy_mileage|hwy_incremented|\n",
      "+---+---------+-----------+---------------+\n",
      "| 29|       30|         29|             30|\n",
      "| 29|       30|         29|             30|\n",
      "| 31|       32|         31|             32|\n",
      "+---+---------+-----------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(\n",
    "    expr(\"hwy\"), # the same as `col`\n",
    "    expr(\"hwy + 1\"), # arithmetic expression col(\"hwy\") + 1\n",
    "    expr(\"hwy AS hwy_mileage\"), # using alias col(\"hwy\").alias(\"hwy_mileage\")\n",
    "    expr(\"hwy + 1 AS hwy_incremented\"), # a combo of the 2 above. \n",
    ").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+\n",
      "|highway|highway|highway|highway|\n",
      "+-------+-------+-------+-------+\n",
      "|     29|     29|     29|     29|\n",
      "|     29|     29|     29|     29|\n",
      "|     31|     31|     31|     31|\n",
      "+-------+-------+-------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(\n",
    "    mpg.hwy.alias(\"highway\"),\n",
    "    col(\"hwy\").alias(\"highway\"),\n",
    "    expr(\"hwy\").alias(\"highway\"),\n",
    "    expr(\"hwy AS highway\")\n",
    ").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL\n",
    "\n",
    "- Spark SQL allows us to write SQL queries against our spark dataframes.  \n",
    "- We'll first \"register\" the table with spark with `sp_df.createOrReplaceTempView('sp_df')`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.createOrReplaceTempView('mpg_view')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we can write a sql query against the mpg table.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+\n",
      "|hwy|cty| avg|\n",
      "+---+---+----+\n",
      "| 29| 18|23.5|\n",
      "| 29| 21|25.0|\n",
      "| 31| 20|25.5|\n",
      "+---+---+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT hwy, cty, (hwy + cty)/2 AS avg\n",
    "    FROM mpg_view\n",
    "    \"\"\"\n",
    ").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The resulting value is another dataframe. \n",
    "- To see the values, we have to ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** All of these methods for creating / manipulating dataframes are the same in terms of performance. The resulting dataframes get turned into the same spark code that gets executed on the JVM, so it really is just a style choice as to which to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type Casting\n",
    "\n",
    "**View column datatypes** using `dtypes` or `printSchema()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('manufacturer', 'string'),\n",
       " ('model', 'string'),\n",
       " ('displ', 'double'),\n",
       " ('year', 'bigint'),\n",
       " ('cyl', 'bigint'),\n",
       " ('trans', 'string'),\n",
       " ('drv', 'string'),\n",
       " ('cty', 'bigint'),\n",
       " ('hwy', 'bigint'),\n",
       " ('fl', 'string'),\n",
       " ('class', 'string')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- manufacturer: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- displ: double (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- cyl: long (nullable = true)\n",
      " |-- trans: string (nullable = true)\n",
      " |-- drv: string (nullable = true)\n",
      " |-- cty: long (nullable = true)\n",
      " |-- hwy: long (nullable = true)\n",
      " |-- fl: string (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To **convert** from one type to another use the `.cast` method on a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hwy: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(mpg.hwy.cast(\"string\")).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a value is not able to be converted, it will be replaced with null. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|model|model|\n",
      "+-----+-----+\n",
      "|   a4| null|\n",
      "|   a4| null|\n",
      "|   a4| null|\n",
      "+-----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(mpg.model, mpg.model.cast(\"int\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Built-in Functions\n",
    "\n",
    "There are many other functions beyond col and expr within the pyspark.sql.functions module for operating on pyspark dataframe columns.\n",
    "\n",
    "- `concat`: to concatenate strings  \n",
    "- `sum`: to sum a group  \n",
    "- `avg`: to take the average of a group  \n",
    "- `min`: to find the minimum  \n",
    "- `max`: to find the maximum  \n",
    "\n",
    "**Note**: importing the sum, min and max functions directly will override the built-in sum, min and max functions. This means you will get an error if you try to sum a list of numbers, because sum will reference the relative pyspark function, which works with pyspark dataframe columns, while the relative built-in function works with lists of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: The pyspark avg and mean functions are aliases of eachother\n",
    "\n",
    "from pyspark.sql.functions import round, concat, sum, min, max, count, avg, mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It very common to see something like:  \n",
    "\n",
    "`import pyspark.sql.functions as F`\n",
    "\n",
    "which will import all of the functions from the `pyspark.sql.functions` module.\n",
    "\n",
    "Try out some functions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+-----------------+--------+--------+\n",
      "|             avg1| avg2|             avg3|min(hwy)|max(hwy)|\n",
      "+-----------------+-----+-----------------+--------+--------+\n",
      "|23.44017094017094|23.44|23.44017094017094|      12|      44|\n",
      "+-----------------+-----+-----------------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(\n",
    "    (sum(mpg.hwy) / count(mpg.hwy)).alias(\"avg1\"),\n",
    "    round(avg(mpg.hwy), 2).alias(\"avg2\"),\n",
    "    mean(mpg.hwy).alias(\"avg3\"),\n",
    "    min(mpg.hwy),\n",
    "    max(mpg.hwy)\n",
    ").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|concat(manufacturer, model)|\n",
      "+---------------------------+\n",
      "|                     audia4|\n",
      "|                     audia4|\n",
      "|                     audia4|\n",
      "+---------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(concat(mpg.manufacturer, mpg.model)).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use a string literal as part of our select, we'll need to use the `lit` function, otherwise spark will try to resolve our string as a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|  cylinders|\n",
      "+-----------+\n",
      "|4 cylinders|\n",
      "|4 cylinders|\n",
      "|4 cylinders|\n",
      "+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "mpg.select(concat(mpg.cyl, lit(\" cylinders\")).alias(\"cylinders\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Manipulation PySpark Functions\n",
    "\n",
    "In order to demonstrate these functions we'll create a dataframe with some text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|address                                      |\n",
      "+---------------------------------------------+\n",
      "|600 Navarro St ste 600, San Antonio, TX 78205|\n",
      "|3130 Broadway St, San Antonio, TX 78209      |\n",
      "|303 Pearl Pkwy, San Antonio, TX 78215        |\n",
      "|1255 SW Loop 410, San Antonio, TX 78227      |\n",
      "+---------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textdf = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"address\": [\n",
    "                \"600 Navarro St ste 600, San Antonio, TX 78205\",\n",
    "                \"3130 Broadway St, San Antonio, TX 78209\",\n",
    "                \"303 Pearl Pkwy, San Antonio, TX 78215\",\n",
    "                \"1255 SW Loop 410, San Antonio, TX 78227\",\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "textdf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`regexp_extract`: specify a regular expression with at least one capture group, and create a new column based on the contents of a capture group.\n",
    "\n",
    "\n",
    "- first argument: the name of the string column to extract from.  \n",
    "- second argument: the regular expression itself.  \n",
    "- last argument: specifies which capture group we want to use. If, for example, our regular expression had 2 capture groups in it and we wanted the contents of the 2nd group, we would specify a 2 here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+---------+------------------+\n",
      "|address                                      |street_no|street            |\n",
      "+---------------------------------------------+---------+------------------+\n",
      "|600 Navarro St ste 600, San Antonio, TX 78205|600      |Navarro St ste 600|\n",
      "|3130 Broadway St, San Antonio, TX 78209      |3130     |Broadway St       |\n",
      "|303 Pearl Pkwy, San Antonio, TX 78215        |303      |Pearl Pkwy        |\n",
      "|1255 SW Loop 410, San Antonio, TX 78227      |1255     |SW Loop 410       |\n",
      "+---------------------------------------------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textdf.select(\"address\", \n",
    "              regexp_extract(\"address\", \n",
    "                             r\"^(\\d+)\", 1).alias(\"street_no\"),\n",
    "              regexp_extract(\"address\", \n",
    "                             r\"^\\d+\\s([\\w\\s]+?),\", 1).alias(\"street\")\n",
    "             ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`regexp_replace` lets us make substitutions based on a regular expression.\n",
    "\n",
    "Below, we obtain just the city, state, and zip code of the address by replacing everything up to the first comma with an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+---------------------+\n",
      "|address                                      |city_state_zip       |\n",
      "+---------------------------------------------+---------------------+\n",
      "|600 Navarro St ste 600, San Antonio, TX 78205|San Antonio, TX 78205|\n",
      "|3130 Broadway St, San Antonio, TX 78209      |San Antonio, TX 78209|\n",
      "|303 Pearl Pkwy, San Antonio, TX 78215        |San Antonio, TX 78215|\n",
      "|1255 SW Loop 410, San Antonio, TX 78227      |San Antonio, TX 78227|\n",
      "+---------------------------------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textdf.select(\"address\",\n",
    "             regexp_replace(\"address\", \n",
    "                           r\"^.*?,\\s*\", \"\").alias(\"city_state_zip\")\n",
    "             ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Subsetting and Filtering of Dataframes\n",
    "\n",
    "`.filter` and `.where` both allow us to select a subset of the rows of our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pandas</th>\n",
       "      <th>spark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>csv</th>\n",
       "      <td>pd.read_csv(\"myfile.csv\")</td>\n",
       "      <td>spark.read.load(\"myfile.csv\", format = \"csv\", sep = \",\")</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>json</th>\n",
       "      <td>pd.read_json(\"myfile.json\")</td>\n",
       "      <td>spark.read.load(\"myfile.json\", format = \"json\") OR spark.read.json(\"myfile.json\")</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st n rows</th>\n",
       "      <td>pd_df.head()</td>\n",
       "      <td>sp_df.show(), .head(), .take()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st row</th>\n",
       "      <td>pd_df.head(1)</td>\n",
       "      <td>sp_df.first()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summary statistics</th>\n",
       "      <td>pd_df.describe()</td>\n",
       "      <td>sp_df.describe()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column names</th>\n",
       "      <td>pd_df.columns</td>\n",
       "      <td>sp_df.columns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># rows</th>\n",
       "      <td>len(pd_df)</td>\n",
       "      <td>sp_df.count()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># distinct rows</th>\n",
       "      <td>len(pd_df.drop_duplicates())</td>\n",
       "      <td>sp_df.distinct().count()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>df schema info</th>\n",
       "      <td>pd_df.info()</td>\n",
       "      <td>sp_df.printSchema()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>select columns</th>\n",
       "      <td>pd_df[[\"col1\", \"col2\"]]</td>\n",
       "      <td>sp_df.select(sp_df.col1, sp_df.col2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conditional filtering</th>\n",
       "      <td>pd_df[pd_df.c1 &gt; 0]</td>\n",
       "      <td>sp_df.filter(df.c1 &gt; 0), sp_df.where(df.c1 &gt; 0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             pandas  \\\n",
       "csv                       pd.read_csv(\"myfile.csv\")   \n",
       "json                    pd.read_json(\"myfile.json\")   \n",
       "1st n rows                             pd_df.head()   \n",
       "1st row                               pd_df.head(1)   \n",
       "summary statistics                 pd_df.describe()   \n",
       "column names                          pd_df.columns   \n",
       "# rows                                   len(pd_df)   \n",
       "# distinct rows        len(pd_df.drop_duplicates())   \n",
       "df schema info                         pd_df.info()   \n",
       "select columns              pd_df[[\"col1\", \"col2\"]]   \n",
       "conditional filtering           pd_df[pd_df.c1 > 0]   \n",
       "\n",
       "                                                                                                   spark  \n",
       "csv                                             spark.read.load(\"myfile.csv\", format = \"csv\", sep = \",\")  \n",
       "json                   spark.read.load(\"myfile.json\", format = \"json\") OR spark.read.json(\"myfile.json\")  \n",
       "1st n rows                                                                sp_df.show(), .head(), .take()  \n",
       "1st row                                                                                    sp_df.first()  \n",
       "summary statistics                                                                      sp_df.describe()  \n",
       "column names                                                                               sp_df.columns  \n",
       "# rows                                                                                     sp_df.count()  \n",
       "# distinct rows                                                                 sp_df.distinct().count()  \n",
       "df schema info                                                                       sp_df.printSchema()  \n",
       "select columns                                                      sp_df.select(sp_df.col1, sp_df.col2)  \n",
       "conditional filtering                                    sp_df.filter(df.c1 > 0), sp_df.where(df.c1 > 0)  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_v_spark = pd_v_spark.append(pd.DataFrame([['pd_df[pd_df.c1 > 0]', 'sp_df.filter(df.c1 > 0), sp_df.where(df.c1 > 0)'],\n",
    "                                            ],\n",
    "                                            index = ['conditional filtering'], \n",
    "                                            columns = ['pandas', 'spark']))\n",
    "pd_v_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `filter()` and `where()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----+----+---+----------+---+---+---+---+----------+\n",
      "|manufacturer|model|displ|year|cyl|     trans|drv|cty|hwy| fl|     class|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+----------+\n",
      "|       honda|civic|  1.6|1999|  4|manual(m5)|  f| 28| 33|  r|subcompact|\n",
      "|       honda|civic|  1.6|1999|  4|  auto(l4)|  f| 24| 32|  r|subcompact|\n",
      "|       honda|civic|  1.6|1999|  4|manual(m5)|  f| 25| 32|  r|subcompact|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.filter(mpg.cyl==4).where(mpg[\"class\"]==\"subcompact\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Assigning of Values\n",
    "\n",
    "Spark => when :  Excel => IF : SQL => CASE...WHEN : Python => numpy.where\n",
    "\n",
    "- Specify a condition, and a value to produce if that condition is true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pandas</th>\n",
       "      <th>spark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>csv</th>\n",
       "      <td>pd.read_csv(\"myfile.csv\")</td>\n",
       "      <td>spark.read.load(\"myfile.csv\", format = \"csv\", sep = \",\")</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>json</th>\n",
       "      <td>pd.read_json(\"myfile.json\")</td>\n",
       "      <td>spark.read.load(\"myfile.json\", format = \"json\") OR spark.read.json(\"myfile.json\")</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st n rows</th>\n",
       "      <td>pd_df.head()</td>\n",
       "      <td>sp_df.show(), .head(), .take()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st row</th>\n",
       "      <td>pd_df.head(1)</td>\n",
       "      <td>sp_df.first()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summary statistics</th>\n",
       "      <td>pd_df.describe()</td>\n",
       "      <td>sp_df.describe()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column names</th>\n",
       "      <td>pd_df.columns</td>\n",
       "      <td>sp_df.columns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># rows</th>\n",
       "      <td>len(pd_df)</td>\n",
       "      <td>sp_df.count()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># distinct rows</th>\n",
       "      <td>len(pd_df.drop_duplicates())</td>\n",
       "      <td>sp_df.distinct().count()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>df schema info</th>\n",
       "      <td>pd_df.info()</td>\n",
       "      <td>sp_df.printSchema()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>select columns</th>\n",
       "      <td>pd_df[[\"col1\", \"col2\"]]</td>\n",
       "      <td>sp_df.select(sp_df.col1, sp_df.col2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conditional filtering</th>\n",
       "      <td>pd_df[pd_df.c1 &gt; 0]</td>\n",
       "      <td>sp_df.filter(df.c1 &gt; 0), sp_df.where(df.c1 &gt; 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conditional assigning</th>\n",
       "      <td>np.where(pd_df.c1.array &gt; 0, \"positive\")</td>\n",
       "      <td>sp_df.select(df.c1, when(df.c1 &gt; 0, \"positive\").alias(\"number_sign\"))</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         pandas  \\\n",
       "csv                                   pd.read_csv(\"myfile.csv\")   \n",
       "json                                pd.read_json(\"myfile.json\")   \n",
       "1st n rows                                         pd_df.head()   \n",
       "1st row                                           pd_df.head(1)   \n",
       "summary statistics                             pd_df.describe()   \n",
       "column names                                      pd_df.columns   \n",
       "# rows                                               len(pd_df)   \n",
       "# distinct rows                    len(pd_df.drop_duplicates())   \n",
       "df schema info                                     pd_df.info()   \n",
       "select columns                          pd_df[[\"col1\", \"col2\"]]   \n",
       "conditional filtering                       pd_df[pd_df.c1 > 0]   \n",
       "conditional assigning  np.where(pd_df.c1.array > 0, \"positive\")   \n",
       "\n",
       "                                                                                                   spark  \n",
       "csv                                             spark.read.load(\"myfile.csv\", format = \"csv\", sep = \",\")  \n",
       "json                   spark.read.load(\"myfile.json\", format = \"json\") OR spark.read.json(\"myfile.json\")  \n",
       "1st n rows                                                                sp_df.show(), .head(), .take()  \n",
       "1st row                                                                                    sp_df.first()  \n",
       "summary statistics                                                                      sp_df.describe()  \n",
       "column names                                                                               sp_df.columns  \n",
       "# rows                                                                                     sp_df.count()  \n",
       "# distinct rows                                                                 sp_df.distinct().count()  \n",
       "df schema info                                                                       sp_df.printSchema()  \n",
       "select columns                                                      sp_df.select(sp_df.col1, sp_df.col2)  \n",
       "conditional filtering                                    sp_df.filter(df.c1 > 0), sp_df.where(df.c1 > 0)  \n",
       "conditional assigning              sp_df.select(df.c1, when(df.c1 > 0, \"positive\").alias(\"number_sign\"))  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_v_spark = pd_v_spark.append(pd.DataFrame([['np.where(pd_df.c1.array > 0, \"positive\")', \n",
    "                                              'sp_df.select(df.c1, when(df.c1 > 0, \"positive\").alias(\"number_sign\"))'],\n",
    "                                            ],\n",
    "                                            index = ['conditional assigning'], \n",
    "                                            columns = ['pandas', 'spark']))\n",
    "pd_v_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If the condition we specified is false, null will be produced.   \n",
    "- Use the `.otherwise` method to specify a value to use if our condition is false  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pandas</th>\n",
       "      <th>spark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>csv</th>\n",
       "      <td>pd.read_csv(\"myfile.csv\")</td>\n",
       "      <td>spark.read.load(\"myfile.csv\", format = \"csv\", sep = \",\")</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>json</th>\n",
       "      <td>pd.read_json(\"myfile.json\")</td>\n",
       "      <td>spark.read.load(\"myfile.json\", format = \"json\") OR spark.read.json(\"myfile.json\")</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st n rows</th>\n",
       "      <td>pd_df.head()</td>\n",
       "      <td>sp_df.show(), .head(), .take()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st row</th>\n",
       "      <td>pd_df.head(1)</td>\n",
       "      <td>sp_df.first()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summary statistics</th>\n",
       "      <td>pd_df.describe()</td>\n",
       "      <td>sp_df.describe()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column names</th>\n",
       "      <td>pd_df.columns</td>\n",
       "      <td>sp_df.columns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># rows</th>\n",
       "      <td>len(pd_df)</td>\n",
       "      <td>sp_df.count()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># distinct rows</th>\n",
       "      <td>len(pd_df.drop_duplicates())</td>\n",
       "      <td>sp_df.distinct().count()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>df schema info</th>\n",
       "      <td>pd_df.info()</td>\n",
       "      <td>sp_df.printSchema()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>select columns</th>\n",
       "      <td>pd_df[[\"col1\", \"col2\"]]</td>\n",
       "      <td>sp_df.select(sp_df.col1, sp_df.col2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conditional filtering</th>\n",
       "      <td>pd_df[pd_df.c1 &gt; 0]</td>\n",
       "      <td>sp_df.filter(df.c1 &gt; 0), sp_df.where(df.c1 &gt; 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conditional assigning</th>\n",
       "      <td>np.where(pd_df.c1.array &gt; 0, \"positive\")</td>\n",
       "      <td>sp_df.select(df.c1, when(df.c1 &gt; 0, \"positive\").alias(\"number_sign\"))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conditional assigning with else</th>\n",
       "      <td>np.where(pd_df.c1.array &gt; 0, \"pos\", \"neg\")</td>\n",
       "      <td>sp_df.select(df.c1, when(df.c1 &gt; 0, \"pos\").otherwise(\"neg\").alias(\"number_sign\"))</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     pandas  \\\n",
       "csv                                               pd.read_csv(\"myfile.csv\")   \n",
       "json                                            pd.read_json(\"myfile.json\")   \n",
       "1st n rows                                                     pd_df.head()   \n",
       "1st row                                                       pd_df.head(1)   \n",
       "summary statistics                                         pd_df.describe()   \n",
       "column names                                                  pd_df.columns   \n",
       "# rows                                                           len(pd_df)   \n",
       "# distinct rows                                len(pd_df.drop_duplicates())   \n",
       "df schema info                                                 pd_df.info()   \n",
       "select columns                                      pd_df[[\"col1\", \"col2\"]]   \n",
       "conditional filtering                                   pd_df[pd_df.c1 > 0]   \n",
       "conditional assigning              np.where(pd_df.c1.array > 0, \"positive\")   \n",
       "conditional assigning with else  np.where(pd_df.c1.array > 0, \"pos\", \"neg\")   \n",
       "\n",
       "                                                                                                             spark  \n",
       "csv                                                       spark.read.load(\"myfile.csv\", format = \"csv\", sep = \",\")  \n",
       "json                             spark.read.load(\"myfile.json\", format = \"json\") OR spark.read.json(\"myfile.json\")  \n",
       "1st n rows                                                                          sp_df.show(), .head(), .take()  \n",
       "1st row                                                                                              sp_df.first()  \n",
       "summary statistics                                                                                sp_df.describe()  \n",
       "column names                                                                                         sp_df.columns  \n",
       "# rows                                                                                               sp_df.count()  \n",
       "# distinct rows                                                                           sp_df.distinct().count()  \n",
       "df schema info                                                                                 sp_df.printSchema()  \n",
       "select columns                                                                sp_df.select(sp_df.col1, sp_df.col2)  \n",
       "conditional filtering                                              sp_df.filter(df.c1 > 0), sp_df.where(df.c1 > 0)  \n",
       "conditional assigning                        sp_df.select(df.c1, when(df.c1 > 0, \"positive\").alias(\"number_sign\"))  \n",
       "conditional assigning with else  sp_df.select(df.c1, when(df.c1 > 0, \"pos\").otherwise(\"neg\").alias(\"number_sign\"))  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_v_spark = pd_v_spark.append(pd.DataFrame([['np.where(pd_df.c1.array > 0, \"pos\", \"neg\")', \n",
    "                                              'sp_df.select(df.c1, when(df.c1 > 0, \"pos\").otherwise(\"neg\").alias(\"number_sign\"))'],\n",
    "                                            ],\n",
    "                                            index = ['conditional assigning with else'], \n",
    "                                            columns = ['pandas', 'spark']))\n",
    "pd_v_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `.when()` with `.otherwise()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "|hwy|   mpg_class|\n",
      "+---+------------+\n",
      "| 29|good_mileage|\n",
      "| 29|good_mileage|\n",
      "| 31|good_mileage|\n",
      "| 30|good_mileage|\n",
      "| 26|good_mileage|\n",
      "| 26|good_mileage|\n",
      "| 27|good_mileage|\n",
      "| 26|good_mileage|\n",
      "| 25| bad_mileage|\n",
      "| 28|good_mileage|\n",
      "+---+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(mpg.hwy,\n",
    "          when(mpg.hwy > 25, \"good_mileage\")\n",
    "          .otherwise(\"bad_mileage\")\n",
    "          .alias(\"mpg_class\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|displ|engine_size|\n",
      "+-----+-----------+\n",
      "|  1.8|      small|\n",
      "|  1.8|      small|\n",
      "|  2.0|     medium|\n",
      "|  2.0|     medium|\n",
      "|  2.8|     medium|\n",
      "|  2.8|     medium|\n",
      "|  3.1|      large|\n",
      "|  1.8|      small|\n",
      "|  1.8|      small|\n",
      "|  2.0|     medium|\n",
      "+-----+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(\n",
    "    mpg.displ,\n",
    "    (when(mpg.displ < 2, \"small\")\n",
    "     .when(mpg.displ < 3, \"medium\")\n",
    "     .otherwise(\"large\")\n",
    "     .alias(\"engine_size\")\n",
    "    )).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To specify multiple conditions, we can chain `.when` calls.   \n",
    "- The first condition that is met will be the value that is used.  \n",
    "- If none of the conditions are met the value specified in the .otherwise will be used (or null if you don't provide a .otherwise).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here that a car with a displ of 1.8 matches both conditions we specified, but small is produced because it is associated with the first matching condition. For any value between 2 and 3, medium will be produced, and anything larger than 3 will produce large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting and Ordering\n",
    "\n",
    "- Sort the rows by one or more columns with two methods: `.sort` and `.orderBy`. \n",
    "- `.sort` and `.orderBy` are aliases of each other and do the exact same thing. \n",
    "- Takes in a Column object or a string that is the name of a column.\n",
    "- By default, values are sorted in ascending order.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-----+----+---+----------+---+---+---+---+------+\n",
      "|manufacturer|              model|displ|year|cyl|     trans|drv|cty|hwy| fl| class|\n",
      "+------------+-------------------+-----+----+---+----------+---+---+---+---+------+\n",
      "|       dodge|        durango 4wd|  4.7|2008|  8|  auto(l5)|  4|  9| 12|  e|   suv|\n",
      "|       dodge|ram 1500 pickup 4wd|  4.7|2008|  8|  auto(l5)|  4|  9| 12|  e|pickup|\n",
      "|       dodge|ram 1500 pickup 4wd|  4.7|2008|  8|manual(m6)|  4|  9| 12|  e|pickup|\n",
      "|        jeep| grand cherokee 4wd|  4.7|2008|  8|  auto(l5)|  4|  9| 12|  e|   suv|\n",
      "|       dodge|  dakota pickup 4wd|  4.7|2008|  8|  auto(l5)|  4|  9| 12|  e|pickup|\n",
      "|   chevrolet|    k1500 tahoe 4wd|  5.3|2008|  8|  auto(l4)|  4| 11| 14|  e|   suv|\n",
      "|        jeep| grand cherokee 4wd|  6.1|2008|  8|  auto(l5)|  4| 11| 14|  p|   suv|\n",
      "|  land rover|        range rover|  4.0|1999|  8|  auto(l4)|  4| 11| 15|  p|   suv|\n",
      "+------------+-------------------+-----+----+---+----------+---+---+---+---+------+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.sort(mpg.hwy).show(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To sort in descending order, we can use the `.desc` method on any Column object, or the `desc` function from `pyspark.sql.functions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-----+----+---+----------+---+---+---+---+----------+\n",
      "|manufacturer|     model|displ|year|cyl|     trans|drv|cty|hwy| fl|     class|\n",
      "+------------+----------+-----+----+---+----------+---+---+---+---+----------+\n",
      "|  volkswagen|new beetle|  1.9|1999|  4|manual(m5)|  f| 35| 44|  d|subcompact|\n",
      "|  volkswagen|     jetta|  1.9|1999|  4|manual(m5)|  f| 33| 44|  d|   compact|\n",
      "|  volkswagen|new beetle|  1.9|1999|  4|  auto(l4)|  f| 29| 41|  d|subcompact|\n",
      "|      toyota|   corolla|  1.8|2008|  4|manual(m5)|  f| 28| 37|  r|   compact|\n",
      "|       honda|     civic|  1.8|2008|  4|  auto(l5)|  f| 24| 36|  c|subcompact|\n",
      "+------------+----------+-----+----+---+----------+---+---+---+---+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import asc, desc\n",
    "\n",
    "mpg.sort(mpg.hwy.desc())\n",
    "# is the same as...\n",
    "mpg.sort(col(\"hwy\").desc())\n",
    "# is the same as ..\n",
    "mpg.sort(desc(\"hwy\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To specify sorting by multiple columns, we provide each column as a separate argument to `.sort`.  \n",
    "\n",
    "In the example below: \n",
    "\n",
    "1. Reverse alphabetically by the vehicle's class   \n",
    "2. By the number of cylinders from lowest to highest  \n",
    "3. By the vehicle's highway mileage, from greatest to smallest  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+\n",
      "|class|cyl|hwy|\n",
      "+-----+---+---+\n",
      "|  suv|  4| 27|\n",
      "|  suv|  4| 26|\n",
      "|  suv|  4| 25|\n",
      "|  suv|  4| 25|\n",
      "|  suv|  4| 24|\n",
      "|  suv|  4| 23|\n",
      "|  suv|  4| 20|\n",
      "|  suv|  4| 20|\n",
      "|  suv|  6| 22|\n",
      "|  suv|  6| 20|\n",
      "|  suv|  6| 20|\n",
      "|  suv|  6| 20|\n",
      "|  suv|  6| 19|\n",
      "|  suv|  6| 19|\n",
      "|  suv|  6| 19|\n",
      "|  suv|  6| 19|\n",
      "|  suv|  6| 19|\n",
      "|  suv|  6| 17|\n",
      "|  suv|  6| 17|\n",
      "|  suv|  6| 17|\n",
      "+-----+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(mpg['class'], \n",
    "           mpg.cyl, \n",
    "           mpg.hwy).sort(desc(\"class\"), mpg.cyl.asc(), col(\"hwy\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pandas</th>\n",
       "      <th>spark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>csv</th>\n",
       "      <td>pd.read_csv(\"myfile.csv\")</td>\n",
       "      <td>spark.read.load(\"myfile.csv\", format = \"csv\", sep = \",\")</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>json</th>\n",
       "      <td>pd.read_json(\"myfile.json\")</td>\n",
       "      <td>spark.read.load(\"myfile.json\", format = \"json\") OR spark.read.json(\"myfile.json\")</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st n rows</th>\n",
       "      <td>pd_df.head()</td>\n",
       "      <td>sp_df.show(), .head(), .take()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st row</th>\n",
       "      <td>pd_df.head(1)</td>\n",
       "      <td>sp_df.first()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summary statistics</th>\n",
       "      <td>pd_df.describe()</td>\n",
       "      <td>sp_df.describe()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column names</th>\n",
       "      <td>pd_df.columns</td>\n",
       "      <td>sp_df.columns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># rows</th>\n",
       "      <td>len(pd_df)</td>\n",
       "      <td>sp_df.count()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># distinct rows</th>\n",
       "      <td>len(pd_df.drop_duplicates())</td>\n",
       "      <td>sp_df.distinct().count()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>df schema info</th>\n",
       "      <td>pd_df.info()</td>\n",
       "      <td>sp_df.printSchema()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>select columns</th>\n",
       "      <td>pd_df[[\"col1\", \"col2\"]]</td>\n",
       "      <td>sp_df.select(sp_df.col1, sp_df.col2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conditional filtering</th>\n",
       "      <td>pd_df[pd_df.c1 &gt; 0]</td>\n",
       "      <td>sp_df.filter(df.c1 &gt; 0), sp_df.where(df.c1 &gt; 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conditional assigning</th>\n",
       "      <td>np.where(pd_df.c1.array &gt; 0, \"positive\")</td>\n",
       "      <td>sp_df.select(df.c1, when(df.c1 &gt; 0, \"positive\").alias(\"number_sign\"))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conditional assigning with else</th>\n",
       "      <td>np.where(pd_df.c1.array &gt; 0, \"pos\", \"neg\")</td>\n",
       "      <td>sp_df.select(df.c1, when(df.c1 &gt; 0, \"pos\").otherwise(\"neg\").alias(\"number_sign\"))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sort 1 col asc</th>\n",
       "      <td>pd_df.sort_values(by=[\"c1\"])</td>\n",
       "      <td>sp_df.sort(sp_df.c1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sort 2+ cols asc</th>\n",
       "      <td>pd_df.sort_values(by=[\"c1\",\"c2\"])</td>\n",
       "      <td>sp_df.sort(sp_df.c1, sp_df.c2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sort 2+ cols desc/asc</th>\n",
       "      <td>pd_df.sort_values(by=[\"c1\",\"c2\"], ascending=[False, True])</td>\n",
       "      <td>sp_df.sort(sp_df.c1.desc(), sp_df.c2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sort 2+ cols desc</th>\n",
       "      <td>pd_df.sort_values(by=[\"c1\",\"c2\"], ascending=False)</td>\n",
       "      <td>sp_df.sort(desc(\"c1\"), desc(\"c2\")) OR sp_df.sort(col(\"c1\").desc(), col(\"c2\").desc())</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                     pandas  \\\n",
       "csv                                                               pd.read_csv(\"myfile.csv\")   \n",
       "json                                                            pd.read_json(\"myfile.json\")   \n",
       "1st n rows                                                                     pd_df.head()   \n",
       "1st row                                                                       pd_df.head(1)   \n",
       "summary statistics                                                         pd_df.describe()   \n",
       "column names                                                                  pd_df.columns   \n",
       "# rows                                                                           len(pd_df)   \n",
       "# distinct rows                                                len(pd_df.drop_duplicates())   \n",
       "df schema info                                                                 pd_df.info()   \n",
       "select columns                                                      pd_df[[\"col1\", \"col2\"]]   \n",
       "conditional filtering                                                   pd_df[pd_df.c1 > 0]   \n",
       "conditional assigning                              np.where(pd_df.c1.array > 0, \"positive\")   \n",
       "conditional assigning with else                  np.where(pd_df.c1.array > 0, \"pos\", \"neg\")   \n",
       "sort 1 col asc                                                 pd_df.sort_values(by=[\"c1\"])   \n",
       "sort 2+ cols asc                                          pd_df.sort_values(by=[\"c1\",\"c2\"])   \n",
       "sort 2+ cols desc/asc            pd_df.sort_values(by=[\"c1\",\"c2\"], ascending=[False, True])   \n",
       "sort 2+ cols desc                        pd_df.sort_values(by=[\"c1\",\"c2\"], ascending=False)   \n",
       "\n",
       "                                                                                                                spark  \n",
       "csv                                                          spark.read.load(\"myfile.csv\", format = \"csv\", sep = \",\")  \n",
       "json                                spark.read.load(\"myfile.json\", format = \"json\") OR spark.read.json(\"myfile.json\")  \n",
       "1st n rows                                                                             sp_df.show(), .head(), .take()  \n",
       "1st row                                                                                                 sp_df.first()  \n",
       "summary statistics                                                                                   sp_df.describe()  \n",
       "column names                                                                                            sp_df.columns  \n",
       "# rows                                                                                                  sp_df.count()  \n",
       "# distinct rows                                                                              sp_df.distinct().count()  \n",
       "df schema info                                                                                    sp_df.printSchema()  \n",
       "select columns                                                                   sp_df.select(sp_df.col1, sp_df.col2)  \n",
       "conditional filtering                                                 sp_df.filter(df.c1 > 0), sp_df.where(df.c1 > 0)  \n",
       "conditional assigning                           sp_df.select(df.c1, when(df.c1 > 0, \"positive\").alias(\"number_sign\"))  \n",
       "conditional assigning with else     sp_df.select(df.c1, when(df.c1 > 0, \"pos\").otherwise(\"neg\").alias(\"number_sign\"))  \n",
       "sort 1 col asc                                                                                   sp_df.sort(sp_df.c1)  \n",
       "sort 2+ cols asc                                                                       sp_df.sort(sp_df.c1, sp_df.c2)  \n",
       "sort 2+ cols desc/asc                                                           sp_df.sort(sp_df.c1.desc(), sp_df.c2)  \n",
       "sort 2+ cols desc                sp_df.sort(desc(\"c1\"), desc(\"c2\")) OR sp_df.sort(col(\"c1\").desc(), col(\"c2\").desc())  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_v_spark = pd_v_spark.append(pd.DataFrame([['pd_df.sort_values(by=[\"c1\"])', \n",
    "                                              'sp_df.sort(sp_df.c1)'],\n",
    "                                             ['pd_df.sort_values(by=[\"c1\",\"c2\"])',\n",
    "                                              'sp_df.sort(sp_df.c1, sp_df.c2)'],\n",
    "                                             ['pd_df.sort_values(by=[\"c1\",\"c2\"], ascending=[False, True])',\n",
    "                                              'sp_df.sort(sp_df.c1.desc(), sp_df.c2)'],\n",
    "                                             ['pd_df.sort_values(by=[\"c1\",\"c2\"], ascending=False)', \n",
    "                                              'sp_df.sort(desc(\"c1\"), desc(\"c2\")) OR sp_df.sort(col(\"c1\").desc(), col(\"c2\").desc())']\n",
    "                                            ],\n",
    "                                            index = ['sort 1 col asc', 'sort 2+ cols asc', 'sort 2+ cols desc/asc', 'sort 2+ cols desc'], \n",
    "                                            columns = ['pandas', 'spark']))\n",
    "pd_v_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping and Aggregating\n",
    "\n",
    "- To aggregate our data by group, use the `.groupBy` method.  \n",
    "- Like with .select and .sort, we can pass either Column objects or strings that are column names to .groupBy.  \n",
    "- All of the expressions below are equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x7fcbe3a126d8>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg.groupBy(mpg.cyl)\n",
    "mpg.groupBy(col('cyl'))\n",
    "mpg.groupBy('cyl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once the data is grouped, specify an aggregation.    \n",
    "- We can use one of the aggregate functions we imported earlier, along with a column  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----------------+\n",
      "|cyl|          avg(cty)|         avg(hwy)|\n",
      "+---+------------------+-----------------+\n",
      "|  6| 16.21518987341772|22.82278481012658|\n",
      "|  5|              20.5|            28.75|\n",
      "|  8|12.571428571428571|17.62857142857143|\n",
      "|  4|21.012345679012345|28.80246913580247|\n",
      "+---+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.groupBy(mpg.cyl).agg(avg(mpg.cty), avg(mpg.hwy)).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To group by multiple columns, pass each of the columns as a separate argument to .groupBy.   \n",
    "- This is different from pandas, where we would need to pass a list.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------------+------------------+\n",
      "|cyl|     class|          avg(cty)|          avg(hwy)|\n",
      "+---+----------+------------------+------------------+\n",
      "|  5|   compact|              21.0|              29.0|\n",
      "|  5|subcompact|              20.0|              28.5|\n",
      "|  6|subcompact|              17.0|24.714285714285715|\n",
      "|  6|    pickup|              14.5|              17.9|\n",
      "|  4|subcompact|22.857142857142858| 30.80952380952381|\n",
      "|  8|       suv|12.131578947368421|16.789473684210527|\n",
      "|  8|    pickup|              11.8|              15.8|\n",
      "|  8|   midsize|              16.0|              24.0|\n",
      "|  4|   midsize|              20.5|           29.1875|\n",
      "|  8|   2seater|              15.4|              24.8|\n",
      "|  6|   compact|16.923076923076923|25.307692307692307|\n",
      "|  6|   minivan|              15.6|              22.2|\n",
      "|  4|   compact|            21.375|          29.46875|\n",
      "|  8|subcompact|              14.8|              21.6|\n",
      "|  6|   midsize|17.782608695652176| 26.26086956521739|\n",
      "|  4|   minivan|              18.0|              24.0|\n",
      "|  4|    pickup|              16.0|20.666666666666668|\n",
      "|  6|       suv|              14.5|              18.5|\n",
      "|  4|       suv|              18.0|             23.75|\n",
      "+---+----------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.groupBy(\"cyl\", \"class\").agg(avg(mpg.cty), avg(mpg.hwy)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In addition to `.groupBy`, we can use `.rollup`, which will do the same aggregations, but will also include the overall total.  \n",
    "- Below the null value in cyl indicates the total count.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| cyl|count|\n",
      "+----+-----+\n",
      "|null|  234|\n",
      "|   4|   81|\n",
      "|   5|    4|\n",
      "|   6|   79|\n",
      "|   8|   70|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.rollup(\"cyl\").count().sort(\"cyl\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use `.rollup` to compute average by group with an overall average\n",
    "- The null row represents the overall average highway mileage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+\n",
      "| cyl|         avg(hwy)|\n",
      "+----+-----------------+\n",
      "|null|23.44017094017094|\n",
      "|   4|28.80246913580247|\n",
      "|   5|            28.75|\n",
      "|   6|22.82278481012658|\n",
      "|   8|17.62857142857143|\n",
      "+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.rollup(\"cyl\").agg(expr(\"avg(hwy)\")).sort(\"cyl\").show()\n",
    "\n",
    "# these are the same...\n",
    "\n",
    "mpg.rollup(\"cyl\").agg(avg(mpg.hwy)).sort(\"cyl\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can rollup to multiple columns.  \n",
    "- Where cyl = null you see the overall average.  \n",
    "- Where cyl = n and class = null, you have the average across all classes for each cylinder value.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------------------+\n",
      "| cyl|     class|          avg(hwy)|\n",
      "+----+----------+------------------+\n",
      "|null|      null| 23.44017094017094|\n",
      "|   4|      null| 28.80246913580247|\n",
      "|   4|   compact|          29.46875|\n",
      "|   4|   midsize|           29.1875|\n",
      "|   4|   minivan|              24.0|\n",
      "|   4|    pickup|20.666666666666668|\n",
      "|   4|subcompact| 30.80952380952381|\n",
      "|   4|       suv|             23.75|\n",
      "|   5|      null|             28.75|\n",
      "|   5|   compact|              29.0|\n",
      "|   5|subcompact|              28.5|\n",
      "|   6|      null| 22.82278481012658|\n",
      "|   6|   compact|25.307692307692307|\n",
      "|   6|   midsize| 26.26086956521739|\n",
      "|   6|   minivan|              22.2|\n",
      "|   6|    pickup|              17.9|\n",
      "|   6|subcompact|24.714285714285715|\n",
      "|   6|       suv|              18.5|\n",
      "|   8|      null| 17.62857142857143|\n",
      "|   8|   2seater|              24.8|\n",
      "+----+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.rollup(\"cyl\", \"class\").mean(\"hwy\").sort(col(\"cyl\"), col(\"class\")\n",
    "                                           ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crosstabs and Pivot Tables  \n",
    "\n",
    "- Another way to aggregate is by `.crosstab`.    \n",
    "- Similar to pandas `.crosstab` function, in that it calculates the number of occurrences of each unique value from the two passed columns.    \n",
    "- `.crosstab` does counts.  \n",
    "- For a different aggregation, use `.pivot`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+---+---+---+\n",
      "| class_cyl|  4|  5|  6|  8|\n",
      "+----------+---+---+---+---+\n",
      "|   midsize| 16|  0| 23|  2|\n",
      "|subcompact| 21|  2|  7|  5|\n",
      "|   2seater|  0|  0|  0|  5|\n",
      "|    pickup|  3|  0| 10| 20|\n",
      "|   minivan|  1|  0| 10|  0|\n",
      "|       suv|  8|  0| 16| 38|\n",
      "|   compact| 32|  2| 13|  0|\n",
      "+----------+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.crosstab(\"class\", \"cyl\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the average highway mileage for each combination of car class and number of cylinders, we could use `.pivot`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+----+----+\n",
      "|     class|   4|   5|   6|   8|\n",
      "+----------+----+----+----+----+\n",
      "|subcompact|  21|   2|   7|   5|\n",
      "|   compact|  32|   2|  13|null|\n",
      "|   minivan|   1|null|  10|null|\n",
      "|       suv|   8|null|  16|  38|\n",
      "|   midsize|  16|null|  23|   2|\n",
      "|    pickup|   3|null|  10|  20|\n",
      "|   2seater|null|null|null|   5|\n",
      "+----------+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.groupBy(\"class\").pivot(\"cyl\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+----+------------------+------------------+\n",
      "|     class|                 4|   5|                 6|                 8|\n",
      "+----------+------------------+----+------------------+------------------+\n",
      "|   2seater|              null|null|              null|              24.8|\n",
      "|   compact|          29.46875|29.0|25.307692307692307|              null|\n",
      "|   midsize|           29.1875|null| 26.26086956521739|              24.0|\n",
      "|   minivan|              24.0|null|              22.2|              null|\n",
      "|    pickup|20.666666666666668|null|              17.9|              15.8|\n",
      "|subcompact| 30.80952380952381|28.5|24.714285714285715|              21.6|\n",
      "|       suv|             23.75|null|              18.5|16.789473684210527|\n",
      "+----------+------------------+----+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.groupBy(\"class\").pivot(\"cyl\").mean(\"hwy\").sort(col(\"class\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how this is a reshape of the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------------------+\n",
      "|     class|cyl|round(avg(hwy), 2)|\n",
      "+----------+---+------------------+\n",
      "|   2seater|  8|              24.8|\n",
      "|   compact|  4|             29.47|\n",
      "|   compact|  5|              29.0|\n",
      "|   compact|  6|             25.31|\n",
      "|   midsize|  8|              24.0|\n",
      "|   midsize|  4|             29.19|\n",
      "|   midsize|  6|             26.26|\n",
      "|   minivan|  4|              24.0|\n",
      "|   minivan|  6|              22.2|\n",
      "|    pickup|  8|              15.8|\n",
      "|    pickup|  4|             20.67|\n",
      "|    pickup|  6|              17.9|\n",
      "|subcompact|  5|              28.5|\n",
      "|subcompact|  8|              21.6|\n",
      "|subcompact|  6|             24.71|\n",
      "|subcompact|  4|             30.81|\n",
      "|       suv|  4|             23.75|\n",
      "|       suv|  8|             16.79|\n",
      "|       suv|  6|              18.5|\n",
      "+----------+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.groupBy(\"class\", \"cyl\").agg(round(mean(\"hwy\"), 2)).sort(col(\"class\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from above:   \n",
    "- The unique values from the column we group by will be the rows in the resulting dataframe.  \n",
    "- The unique values from the column we pivot on will become the columns.  \n",
    "- The values in each cell will be equal to the aggregation we specified over the group of values defined by the intersection of the rows and the columns.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Data  \n",
    "\n",
    "Let's take a look at how spark handles missing data. First we'll create a dataframe that has a few missing values:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|1.0|NaN|\n",
      "|2.0|0.0|\n",
      "|NaN|0.0|\n",
      "|4.0|3.0|\n",
      "|5.0|1.0|\n",
      "|NaN|NaN|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        {\"x\": [1, 2, np.nan, 4, 5, np.nan], \"y\": [np.nan, 0, 0, 3, 1, np.nan]}\n",
    "    )\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark provides two main ways to deal with missing values:\n",
    "\n",
    "- `.fill`: to replace missing values with a specified value  \n",
    "- `.drop`: to drop rows containing missing values  \n",
    "- Both methods are accessed through the `.na` property. We'll look at some examples below:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|2.0|0.0|\n",
      "|4.0|3.0|\n",
      "|5.0|1.0|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|1.0|0.0|\n",
      "|2.0|0.0|\n",
      "|0.0|0.0|\n",
      "|4.0|3.0|\n",
      "|5.0|1.0|\n",
      "|0.0|0.0|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both methods, we can specify that we only want to fill or drop values in a specific column with a second argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|  x|   y|\n",
      "+---+----+\n",
      "|1.0|-1.0|\n",
      "|2.0| 0.0|\n",
      "|0.0| 0.0|\n",
      "|4.0| 3.0|\n",
      "|5.0| 1.0|\n",
      "|0.0|-1.0|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(0, subset=\"x\").na.fill(-1, subset=\"y\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that above the na values in the x column were filled with 0, but the na values in y were left alone.\n",
    "\n",
    "In the example below, the rows with an na value for the y column will be dropped, but the rows with na values for only the x column will remain.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|2.0|0.0|\n",
      "|NaN|0.0|\n",
      "|4.0|3.0|\n",
      "|5.0|1.0|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(subset=\"y\").show()\n",
    "\n",
    "# df.na.drop(subset=\"price\").na.fill(0, subset=\"items\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Transformations\n",
    "\n",
    "The .explain method will show us how spark is thinking about our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Scan ExistingRDD[manufacturer#108,model#109,displ#110,year#111L,cyl#112L,trans#113,drv#114,cty#115L,hwy#116L,fl#117,class#118]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our basic example, we see that there is only a single step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [cyl#112L, hwy#116L]\n",
      "+- *(1) Scan ExistingRDD[manufacturer#108,model#109,displ#110,year#111L,cyl#112L,trans#113,drv#114,cty#115L,hwy#116L,fl#117,class#118]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(mpg.cyl, mpg.hwy).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are doing a more advanced select calculation, but this is still just a single step to spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(cyl#112L) AND (cyl#112L = 6))\n",
      "+- *(1) Scan ExistingRDD[manufacturer#108,model#109,displ#110,year#111L,cyl#112L,trans#113,drv#114,cty#115L,hwy#116L,fl#117,class#118]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.filter(mpg.cyl == 6).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that our filter is also a single step.\n",
    "\n",
    "Without reading ahead, do you think the execution plan for the two dataframes below will be the same or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [cyl#112L, hwy#116L]\n",
      "+- *(1) Filter (isnotnull(cyl#112L) AND (cyl#112L = 6))\n",
      "   +- *(1) Scan ExistingRDD[manufacturer#108,model#109,displ#110,year#111L,cyl#112L,trans#113,drv#114,cty#115L,hwy#116L,fl#117,class#118]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(\"cyl\", \"hwy\").filter(expr(\"cyl == 6\")).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [cyl#112L, hwy#116L]\n",
      "+- *(1) Filter (isnotnull(cyl#112L) AND (cyl#112L = 6))\n",
      "   +- *(1) Scan ExistingRDD[manufacturer#108,model#109,displ#110,year#111L,cyl#112L,trans#113,drv#114,cty#115L,hwy#116L,fl#117,class#118]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.filter(expr(\"cyl == 6\")).select(\"cyl\", \"hwy\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that even though we specified the transformations (.select and .filter) in a different order, we end up with the same output when we call .explain. This is because spark will look at our dataframe and transform it into the most efficient representation possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [hwy#116L]\n",
      "+- *(1) Scan ExistingRDD[manufacturer#108,model#109,displ#110,year#111L,cyl#112L,trans#113,drv#114,cty#115L,hwy#116L,fl#117,class#118]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.selectExpr(\"cyl + 3*16/4 + 19 AS unused\", \"hwy\").select(\"hwy\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here that we have 2 seperate select statements, but spark will condense this down to a single Project, as it is smart enough to realize that it doesn't actually need to do all the arithmetic we specified in the first select, since we arent using that value later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[], functions=[min(cyl#112L)])\n",
      "+- Exchange SinglePartition, true, [id=#1246]\n",
      "   +- *(1) HashAggregate(keys=[], functions=[partial_min(cyl#112L)])\n",
      "      +- *(1) Project [cyl#112L]\n",
      "         +- *(1) Scan ExistingRDD[manufacturer#108,model#109,displ#110,year#111L,cyl#112L,trans#113,drv#114,cty#115L,hwy#116L,fl#117,class#118]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(min(mpg.cyl)).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice now that the execution plan gets much more complicated. This is because in steps prior, we were applying transformations that applied to each row individually. To calculate a minimum, we have to look at all the rows in the dataset to find the smallest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[cyl#112L], functions=[min(hwy#116L), max(hwy#116L)])\n",
      "+- Exchange hashpartitioning(cyl#112L, 200), true, [id=#1267]\n",
      "   +- *(1) HashAggregate(keys=[cyl#112L], functions=[partial_min(hwy#116L), partial_max(hwy#116L)])\n",
      "      +- *(1) Project [cyl#112L, hwy#116L]\n",
      "         +- *(1) Scan ExistingRDD[manufacturer#108,model#109,displ#110,year#111L,cyl#112L,trans#113,drv#114,cty#115L,hwy#116L,fl#117,class#118]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.groupBy(mpg.cyl).agg(min(mpg.hwy), max(mpg.hwy)).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[cyl#112L], functions=[min(avg_mpg#2968), avg(avg_mpg#2968), max(avg_mpg#2968)])\n",
      "+- Exchange hashpartitioning(cyl#112L, 200), true, [id=#1292]\n",
      "   +- *(1) HashAggregate(keys=[cyl#112L], functions=[partial_min(avg_mpg#2968), partial_avg(avg_mpg#2968), partial_max(avg_mpg#2968)])\n",
      "      +- *(1) Project [cyl#112L, (cast((cty#115L + hwy#116L) as double) / 2.0) AS avg_mpg#2968]\n",
      "         +- *(1) Filter (isnotnull(class#118) AND (class#118 = compact))\n",
      "            +- *(1) Scan ExistingRDD[manufacturer#108,model#109,displ#110,year#111L,cyl#112L,trans#113,drv#114,cty#115L,hwy#116L,fl#117,class#118]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    mpg.select(col(\"cyl\"), expr(\"(cty+hwy)/2 AS avg_mpg\"))\n",
    "    .filter(expr('class == \"compact\"'))\n",
    "    .groupBy(\"cyl\")\n",
    "    .agg(min(\"avg_mpg\"), avg(\"avg_mpg\"), max(\"avg_mpg\"))\n",
    "    .explain()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Dataframe Manipulation Examples\n",
    "\n",
    "Let's take a look at some more examples of working with spark dataframes. For these examples, we'll be working with a dataset of observations of the weather in seattle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------+--------+----+-------+\n",
      "|      date|precipitation|temp_max|temp_min|wind|weather|\n",
      "+----------+-------------+--------+--------+----+-------+\n",
      "|2012-01-01|          0.0|    12.8|     5.0| 4.7|drizzle|\n",
      "|2012-01-02|         10.9|    10.6|     2.8| 4.5|   rain|\n",
      "|2012-01-03|          0.8|    11.7|     7.2| 2.3|   rain|\n",
      "|2012-01-04|         20.3|    12.2|     5.6| 4.7|   rain|\n",
      "|2012-01-05|          1.3|     8.9|     2.8| 6.1|   rain|\n",
      "|2012-01-06|          2.5|     4.4|     2.2| 2.2|   rain|\n",
      "+----------+-------------+--------+--------+----+-------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from vega_datasets import data\n",
    "\n",
    "weather = data.seattle_weather().assign(date=lambda df: df.date.astype(str))\n",
    "weather = spark.createDataFrame(weather)\n",
    "weather.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first find the dates where the data starts and stops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2012-01-01', '2015-12-31')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_date, max_date = weather.select(min(\"date\"), max(\"date\")).first()\n",
    "min_date, max_date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `.select` to select the minimum date and the maximum date. \n",
    "- `.first` returns us the first row of our results, which consists of two values, and so can be unpacked into the min_date and max_date variables.  \n",
    "- Combine the temp max and min columns into a single column, temp_avg.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----+-------+--------+\n",
      "|      date|precipitation|wind|weather|temp_avg|\n",
      "+----------+-------------+----+-------+--------+\n",
      "|2012-01-01|          0.0| 4.7|drizzle|     9.0|\n",
      "|2012-01-02|         10.9| 4.5|   rain|     6.5|\n",
      "|2012-01-03|          0.8| 2.3|   rain|     9.5|\n",
      "|2012-01-04|         20.3| 4.7|   rain|     9.0|\n",
      "|2012-01-05|          1.3| 6.1|   rain|     6.0|\n",
      "|2012-01-06|          2.5| 2.2|   rain|     3.5|\n",
      "+----------+-------------+----+-------+--------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather = weather.withColumn(\n",
    "    \"temp_avg\", expr(\"ROUND(temp_min + temp_max) / 2\")\n",
    ").drop(\"temp_max\", \"temp_min\")\n",
    "weather.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will calculate the total amount of rainfall for each month. We'll do this by first creating a month column, then grouping by the month, and finally, aggregating by taking the sum of the precipitation. To do this we will need to use the month function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|month|    total_rainfall|\n",
      "+-----+------------------+\n",
      "|    1|465.99999999999994|\n",
      "|    2|             422.0|\n",
      "|    3|             606.2|\n",
      "|    4|             375.4|\n",
      "|    5|             207.5|\n",
      "|    6|             132.9|\n",
      "|    7|              48.2|\n",
      "|    8|             163.7|\n",
      "|    9|235.49999999999997|\n",
      "|   10|             503.4|\n",
      "|   11|             642.5|\n",
      "|   12| 622.7000000000002|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import month, year, quarter\n",
    "\n",
    "(\n",
    "    weather.withColumn(\"month\", month(\"date\"))\n",
    "    .groupBy(\"month\")\n",
    "    .agg(sum(\"precipitation\").alias(\"total_rainfall\"))\n",
    "    .sort(\"month\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now take a look at the average tempurature for each type of weather in December 2013:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|weather|    avg(temp_avg)|\n",
      "+-------+-----------------+\n",
      "|    fog|7.555555555555555|\n",
      "|    sun|2.977272727272727|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    weather.filter(month(\"date\") == 12)\n",
    "    .filter(year(\"date\") == 2013)\n",
    "    .groupBy(\"weather\")\n",
    "    .agg(mean(\"temp_avg\"))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we first have a couple of .filter calls in order to restrict our data to December of 2013. We then group by the weather column, and lastly, aggregate by taking the average of our temp_avg column. The combination of group by and agg will calculate the average tempurature for each unique value of the weather column.\n",
    "\n",
    "Let's now find out how many days had freezing tempuratures in each month of 2013.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------+\n",
      "|month|no_days_with_freezing_temps|\n",
      "+-----+---------------------------+\n",
      "|    1|                          3|\n",
      "|    2|                          0|\n",
      "|    3|                          0|\n",
      "|    4|                          0|\n",
      "|    5|                          0|\n",
      "|    6|                          0|\n",
      "|    7|                          0|\n",
      "|    8|                          0|\n",
      "|    9|                          0|\n",
      "|   10|                          0|\n",
      "|   11|                          0|\n",
      "|   12|                          5|\n",
      "+-----+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    weather.filter(year(\"date\") == 2013)\n",
    "    .withColumn(\"freezing_temps\", (weather.temp_avg <= 0).cast(\"int\"))\n",
    "    .withColumn(\"month\", month(\"date\"))\n",
    "    .groupBy(\"month\")\n",
    "    .agg(sum(\"freezing_temps\").alias(\"no_days_with_freezing_temps\"))\n",
    "    .sort(\"month\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(\n",
    "    weather.filter(year(\"date\") == 2013)\n",
    "    .withColumn(\"freezing_temps\", (weather.temp_avg <= 0).cast(\"int\"))\n",
    "    .withColumn(\"month\", month(\"date\"))\n",
    "    .groupBy(\"month\")\n",
    "    .agg(sum(\"freezing_temps\").alias(\"no_of_days_with_freezing_temps\"))\n",
    "    .sort(\"month\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joins\n",
    "Like pandas and sql, spark has functionality that lets us combine two tabular datasets, known as a join.\n",
    "\n",
    "We'll start by creating some data that we can join together:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- users ---\n",
      "+---+-----+-------+\n",
      "| id| name|role_id|\n",
      "+---+-----+-------+\n",
      "|  1|  bob|    1.0|\n",
      "|  2|  joe|    2.0|\n",
      "|  3|sally|    3.0|\n",
      "|  4| adam|    3.0|\n",
      "|  5| jane|    NaN|\n",
      "|  6| mike|    NaN|\n",
      "+---+-----+-------+\n",
      "\n",
      "--- roles ---\n",
      "+---+---------+\n",
      "| id|     name|\n",
      "+---+---------+\n",
      "|  1|    admin|\n",
      "|  2|   author|\n",
      "|  3| reviewer|\n",
      "|  4|commenter|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"id\": [1, 2, 3, 4, 5, 6],\n",
    "            \"name\": [\"bob\", \"joe\", \"sally\", \"adam\", \"jane\", \"mike\"],\n",
    "            \"role_id\": [1, 2, 3, 3, np.nan, np.nan],\n",
    "        }\n",
    "    )\n",
    ")\n",
    "roles = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"id\": [1, 2, 3, 4],\n",
    "            \"name\": [\"admin\", \"author\", \"reviewer\", \"commenter\"],\n",
    "        }\n",
    "    )\n",
    ")\n",
    "print(\"--- users ---\")\n",
    "users.show()\n",
    "print(\"--- roles ---\")\n",
    "roles.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To join two dataframes together, we'll need to call the `.join` method on one of them and supply the other as an argument.  \n",
    "- In addition, we'll need to supply the condition on which we are joining.   \n",
    "- In this case, we are joining where the role_id column on the users table is equal to the id column on the roles table.  \n",
    "- By default, spark will perform an inner join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+---+--------+\n",
      "| id| name|role_id| id|    name|\n",
      "+---+-----+-------+---+--------+\n",
      "|  1|  bob|    1.0|  1|   admin|\n",
      "|  3|sally|    3.0|  3|reviewer|\n",
      "|  4| adam|    3.0|  3|reviewer|\n",
      "|  2|  joe|    2.0|  2|  author|\n",
      "+---+-----+-------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users.join(roles, on=users.role_id == roles.id).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+----+--------+\n",
      "| id| name|role_id|  id|    name|\n",
      "+---+-----+-------+----+--------+\n",
      "|  5| jane|    NaN|null|    null|\n",
      "|  6| mike|    NaN|null|    null|\n",
      "|  1|  bob|    1.0|   1|   admin|\n",
      "|  3|sally|    3.0|   3|reviewer|\n",
      "|  4| adam|    3.0|   3|reviewer|\n",
      "|  2|  joe|    2.0|   2|  author|\n",
      "+---+-----+-------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users.join(roles, on=users.role_id == roles.id, how=\"left\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice a duplicate id column. There are several ways we could go about dealing with this:\n",
    "\n",
    "- alias each dataframe + explicitly select columns after joining (this could also be implemented with spark SQL).  \n",
    "- rename duplicated columns before merging.  \n",
    "- drop duplicated columns after the merge (.drop(right.id))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization (or Lack Therof)\n",
    "\n",
    "Spark does not provide a way to do visualization with their dataframes. To visualize data from spark, you should use the `.toPandas` method on a spark dataframe to convert it to a pandas dataframe, then visualize as you normally would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>role_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>bob</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>joe</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>sally</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>adam</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>jane</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>mike</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   name  role_id\n",
       "0   1    bob      1.0\n",
       "1   2    joe      2.0\n",
       "2   3  sally      3.0\n",
       "3   4   adam      3.0\n",
       "4   5   jane      NaN\n",
       "5   6   mike      NaN"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
